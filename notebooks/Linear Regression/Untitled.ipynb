{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d9cafd8-eb2d-494e-8c45-49f96494fe61",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eda6bf-fff7-4a5d-84ed-5357bbac8869",
   "metadata": {},
   "source": [
    "Using linear regression is usually not good for classification problems because the data usually don't fit a line. Instead, a very roboust algorithm in this situation is called the logistic regression, in which that we assume that our model's prediction satisfies $$h_{\\theta} (x) \\in [0, 1]$$ and that we want the prediction to fit like a sigmoid function instead of just a straight line $$h_{\\theta} (\\mathbf{x}) = \\frac{1}{1 + e^{-\\theta^T \\mathbf{x}}}$$\n",
    "\n",
    "Furthermore, we assume that our data can **only** be $0$ or $1$:\n",
    "- $P(y=1 \\ |\\  \\mathbf{x}; \\theta) = h_{\\theta} (\\mathbf{x})$\n",
    "- $P(y=0 \\ |\\  \\mathbf{x}; \\theta) = 1 - h_{\\theta} (\\mathbf{x})$\n",
    "\n",
    "From these assumptions, it can be shown that we want to find parameter $\\theta$ to maximize the **log likelihood**: $$\n",
    "l(\\theta) = \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc282219-9377-46f6-8f7f-f4798ac4636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3546c81-03f1-4bb3-84dd-4ec477fd5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(theta, x):\n",
    "    \"\"\"\n",
    "    Compute the hypothesis\n",
    "    Args:\n",
    "        theta: Shape (n+1,).\n",
    "        x:     All training examples of shape (m, n+1).\n",
    "    \n",
    "    Return: The hypothesis for all training examples. Shape (m,).\n",
    "\n",
    "    Complexity: O(m * n)\n",
    "    \"\"\"\n",
    "    S = - x @ theta\n",
    "\n",
    "    return 1/(1 + np.exp(S))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f077fa-b039-4959-8a4d-12560b668aef",
   "metadata": {},
   "source": [
    "### Variables:\n",
    "* $n$ is the number of features.\n",
    "* $m$ is the number of training examples.\n",
    "* $\\mathbf{\\theta}$ is the parameters, an $(n+1)$-dimensional vector, where $\\mathbf{\\theta}_0$ is the bias.\n",
    "* $\\mathbf{x}^{(i)}$ is the features, an $(n+1)$-dimensional vector.\n",
    "* $y^{(i)}$ is the actual target value for the $i^{th}$ example.\n",
    "* $J(\\theta)$ is the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314996d1-1d78-4735-bfeb-ac698fb474bb",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "We can use  Newton's method to implement logistic regression. Let $f$ be a real value function. Newton's method find zeros of the function $f(\\theta)$ by iteratively performs the update: $$ \\theta := \\theta - \\frac{f(\\theta)}{f'(\\theta)} $$ \n",
    "\n",
    "Since the maxima of the log likelihood function $l$ corresponds to points where $l'(\\theta) = 0$, we can use Newton's method to find the maxima of $l$ by setting $f = l'$, giving the update rule: $$\\theta = \\theta - \\frac{l'(\\theta)}{l''(\\theta)}$$\n",
    "\n",
    "More generally, the update formula for multi-dimensional Newton's formula is $$\n",
    "\\theta := \\theta - H^{-1} \\nabla_{\\theta} l(\\theta)\n",
    "$$\n",
    "where $$\n",
    "H_{ij} = \\frac{\\partial^2 l(\\theta)}{\\partial \\theta_i \\partial \\theta_j}\n",
    "$$\n",
    "is the Hessian matrix.\n",
    "\n",
    "\n",
    "Advantage:\n",
    "- Compare to gradient descant/ascent, the Newton's method takes a bigger jump each step.\n",
    "- Quadric convergence\n",
    "\n",
    "Disadvantage:\n",
    "- In higher dimension, each steps require inverting a $(n+1), (n+1)$ matrix, so it become very computational expensive.\n",
    "\n",
    "But overall, if there is around **10 - 50** parameters, Newton's method is a very good way to implement logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1c3e3c-60b7-4d0f-a943-64cb5ce20d93",
   "metadata": {},
   "source": [
    "## Hessian matrix for the log likelihood function\n",
    "Recall we have log likelihood function $$\n",
    "l(\\theta) = \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right]\n",
    "$$\n",
    "The Hessian matrix of the log likelihood function can be computed as $$\n",
    "H = - \\sum_{i=1}^{m} h_{\\theta}(x^{(i)})(1 - h_{\\theta}(x^{(i)})) x^{(i)} (x^{(i)})^T\n",
    "$$\n",
    "\n",
    "Note:\n",
    "This formula for Hessian does not depends on y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b091f-fc5c-4abd-85bb-5046859ea10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(theta, x):\n",
    "    \"\"\"\n",
    "    Compute the Hessian matrix for the log likelihood function l.\n",
    "    Args:\n",
    "        theta: Shape (n+1,).\n",
    "        x:     All training examples of shape (m, n+1).\n",
    "\n",
    "    Return: The Hessian matrix. Shape (n+1, n+1).\n",
    "    \"\"\"\n",
    "    h = h(theta, x)\n",
    "    H = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        H = H - h[i] * (1-h[i]) * np.dot(x[i], x[i])\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03cfbf-4ebf-47ed-8a42-5ede9e4465e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
