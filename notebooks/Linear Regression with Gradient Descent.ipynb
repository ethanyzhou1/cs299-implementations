{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6afae787-cc02-4df5-bc46-0ee0ab7149f5",
   "metadata": {},
   "source": [
    "# Linear Regression Using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eacc5ae-7ca7-4a54-9de6-01264b923190",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069be2a-521b-4800-ae2e-ec1569033fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284689f-e8ab-4193-aae3-32a23e2d40fb",
   "metadata": {},
   "source": [
    "### The Least-Squares Cost Function\n",
    "\n",
    "The cost function chosen here is the least-squares cost function. The formula is:\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Note:\n",
    "* The sum of square is a good choice of cost function because in this case, the local minimum of J is the same as the global minimum of J.\n",
    "\n",
    "Where:\n",
    "* $\\frac{1}{2}$ is the standard coefficient.\n",
    "* $J(\\theta)$ is the cost function.\n",
    "* $n$ is the number of features.\n",
    "* $m$ is the number of training examples.\n",
    "* $\\mathbf{x}^{(i)}$ is the features, an $(n+1)$-dimensional vector, where we set $x_0 = 1$.\n",
    "* $\\mathbf{\\theta}$ is the parameters, an $(n+1)$-dimensional vector, where $\\mathbf{\\theta}_0$ is the bias.\n",
    "* $h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}) = \\mathbf{\\theta} \\cdot \\mathbf{x}^{(i)}$ is the model's prediction for the $i^{th}$ example.\n",
    "* $y^{(i)}$ is the actual target value for the $i^{th}$ example.\n",
    "\n",
    "Complexity:\n",
    "* Time: $O(m*n)$\n",
    "* Space: $O(1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bf77df-9b42-474d-a4cb-689396356a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_least_square_cost(x: ndarray, y: ndarray, theta: ndarray):\n",
    "    \"\"\"\n",
    "    Computes the least-squares cost function J\n",
    "    Args:\n",
    "        x (ndarray (m,n+1))       : features, m examples, n features \n",
    "        y (ndarray (m, ))         : target values, m examples\n",
    "        theta (ndarray (n+1, ))   : parameters [theta_0 (bias), theta_1, ... theta_n]\n",
    "\n",
    "    Returns:\n",
    "        float: The value of the cost function J.\n",
    "\n",
    "    Complexity:\n",
    "        Time: O(m*n)\n",
    "        Space: O(1) auxiliary\n",
    "\n",
    "    Notes:\n",
    "        This implementation uses a for-loop for readability. A faster computation\n",
    "        can be achieved using NumPy's vector operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    m, = x.shape\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        h = theta @ x[i] # Prediction. Time: O(n)\n",
    "        cost = cost + (h - y[i])**2 \n",
    "\n",
    "    cost = (1/2) * cost\n",
    "    return cost\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c48f81b-c4ff-43b6-9587-3c9ccdca10a4",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "- [] Create an optimized compute_least_square_cost_vectorized function that uses NumPy operations instead of a for-loop. The current version should be kept for its readability.\n",
    "- [] Might consider using Mean Square Error (MSE) with coefficient $\\frac{1}{2m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664fe9a-bff6-491a-a57b-46c776f9129f",
   "metadata": {},
   "source": [
    "## Gradient of the Least-Squares Cost function\n",
    "\n",
    "Here we compute the gradient of the least square cost function. The formula is:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_j} = \\sum_{i=1}^{m} \\left( \\  (h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \\  \\right)\n",
    "$$\n",
    "\n",
    "Note:\n",
    "* As we chooe the coefficient $\\frac{1}{2}$ in our least square formula, we can avoid a coefficient in the gradient.\n",
    "\n",
    "Where:\n",
    "* $J(\\theta)$ is the cost function.\n",
    "* $n$ is the number of features.\n",
    "* $m$ is the number of training examples.\n",
    "* $\\mathbf{x}^{(i)}$ is the features, an $(n+1)$-dimensional vector, where we set $x_0 = 1$.\n",
    "* $\\mathbf{\\theta}$ is the parameters, an $(n+1)$-dimensional vector, where $\\mathbf{\\theta}_0$ is the bias.\n",
    "* $h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}) = \\mathbf{\\theta} \\cdot \\mathbf{x}^{(i)}$ is the model's prediction for the $i^{th}$ example.\n",
    "* $y^{(i)}$ is the actual target value for the $i^{th}$ example.\n",
    "\n",
    "Implementation Note:\n",
    "* We want to swap i and j in the nested the for-loop to avoid duplicate computation of h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3704e6-6f48-4fe0-81cd-08175cf93998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_least_square_gradient(x: ndarray, y: ndarray, theta: ndarray)\n",
    "    \"\"\"\n",
    "    Computes the gradient for the least square cost function J.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray (m,n+1))       : features, m examples, n features \n",
    "        y (ndarray (m,))         : target values, m examples\n",
    "        theta (ndarray (n+1,))   : parameters [theta_0 (bias), theta_1, ... theta_n]\n",
    "\n",
    "    Return:\n",
    "        (ndarray (n+1,)): n+1 coefficients for the gradient vector of J\n",
    "    \n",
    "    Complexity:\n",
    "        Time: O(m*n)\n",
    "        Space: O(1) auxiliary\n",
    "\n",
    "    Notes:\n",
    "        This implementation uses a for-loop for readability. A faster computation\n",
    "        can be achieved using NumPy's vector operation.\n",
    "    \"\"\"\n",
    "\n",
    "    m, num_parameters = x.shape\n",
    "    dj_dtheta = np.zeros(num_parameters) # Gradient vector of J\n",
    "\n",
    "    # The loop i and j are swapped to reduced duplicate computation of h\n",
    "    for i in range(m):\n",
    "        h = theta @ x[i] # Prediction. Time: O(n)\n",
    "        \n",
    "        for j in range(num_parameters):\n",
    "            dj_dtheta[j] = (h - y[i]) * x[i, j]\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3569e1c-eef3-45d6-92b4-2333abf26e5f",
   "metadata": {},
   "source": [
    "**Inefficient Implementation (For Reference):**\n",
    "```python\n",
    "\"\"\"\n",
    "put i as the inner loop add duplicate computation of h\n",
    "\"\"\"\n",
    "for j in range(num_parameters):\n",
    "    for i in range(m):\n",
    "        h = theta @ x[i]\n",
    "        dj_dtheta[j] = (dj_dtheta[j] - y[i]) * x[i,j]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca57215-1821-4257-85a3-352f5e367be3",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "- [] Note that if we invoke both the compute_least_square_cost and compute_least_square_gradient function in each iteration of gradient descent, we need to do the expensive computation $\\mathbf{\\theta} \\cdot \\mathbf{x}^{(i)} - y^{(i)}$ twice. Can improve this linear regression algorithm by using a different structure to avoid the duplicate computation.\n",
    "- Create an optimized compute_least_square_cost_vectorized function that uses NumPy operations instead of a for-loop. The current version should be kept for its readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1922b-b4a6-429d-bc7e-c9de612265eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs299-projects",
   "language": "python",
   "name": "cs299-projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
